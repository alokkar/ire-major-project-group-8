{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1980,
     "status": "ok",
     "timestamp": 1547340412889,
     "user": {
      "displayName": "Alison Ribeiro",
      "photoUrl": "https://lh6.googleusercontent.com/-p2L5zKSs9Oc/AAAAAAAAAAI/AAAAAAAAAEE/PzwO-PqNro8/s64/photo.jpg",
      "userId": "16421844834226782203"
     },
     "user_tz": 120
    },
    "id": "QL28WSHoBsfp",
    "outputId": "f22d55d4-e94a-4a41-e258-28a2a2077a73"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPool2D, Reshape\n",
    "from keras.layers import Input, concatenate, Dropout\n",
    "from keras.layers import Embedding, Concatenate\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0s3Dnnk8FF4j"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train_en.tsv', delimiter='\\t',encoding='utf-8')\n",
    "dev = pd.read_csv('./dev_en.tsv', delimiter='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lrslbThJCeSY"
   },
   "outputs": [],
   "source": [
    "def clean_tweets(tweet):\n",
    "    tweet = re.sub('@(\\\\w{1,15})\\b', '', tweet)\n",
    "    tweet = tweet.replace(\"via \", \"\")\n",
    "    tweet = tweet.replace(\"RT \", \"\")\n",
    "    tweet = tweet.lower()\n",
    "    return tweet\n",
    "    \n",
    "def clean_url(tweet):\n",
    "    tweet = re.sub('http\\\\S+', '', tweet, flags=re.MULTILINE)   \n",
    "    return tweet\n",
    "    \n",
    "def remove_stop_words(tweet):\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    stops.update(['.',',','\"',\"'\",'?',':',';','(',')','[',']','{','}'])\n",
    "    toks = [tok for tok in tweet if not tok in stops and len(tok) >= 3]\n",
    "    return toks\n",
    "    \n",
    "def stemming_tweets(tweet):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in tweet]\n",
    "    return stemmed_words\n",
    "\n",
    "def remove_number(tweet):\n",
    "    newTweet = re.sub('\\\\d+', '', tweet)\n",
    "    return newTweet\n",
    "\n",
    "def remove_hashtags(tweet):\n",
    "    result = ''\n",
    "\n",
    "    for word in tweet.split():\n",
    "        if word.startswith('#') or word.startswith('@'):\n",
    "            result += word[1:]\n",
    "            result += ' '\n",
    "        else:\n",
    "            result += word\n",
    "            result += ' '\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/aniket.joshi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QDsnLDRQGl71"
   },
   "outputs": [],
   "source": [
    "def preprocessing(tweet, swords = True, url = True, stemming = True, ctweets = True, number = True, hashtag = True):\n",
    "\n",
    "    if ctweets:\n",
    "        tweet = clean_tweets(tweet)\n",
    "\n",
    "    if url:\n",
    "        tweet = clean_url(tweet)\n",
    "\n",
    "    if hashtag:\n",
    "        tweet = remove_hashtags(tweet)\n",
    "    \n",
    "    twtk = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "    if number:\n",
    "        tweet = remove_number(tweet)\n",
    "    \n",
    "    tokens = [w.lower() for w in twtk.tokenize(tweet) if w != \"\" and w is not None]\n",
    "\n",
    "    if swords:\n",
    "        tokens = remove_stop_words(tokens)\n",
    "\n",
    "    if stemming:\n",
    "        tokens = stemming_tweets(tokens)\n",
    "\n",
    "    text = \" \".join(tokens)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SunHFjyyFLR3"
   },
   "outputs": [],
   "source": [
    "train_text  = train['text'].map(lambda x: preprocessing(x, swords = False, url = True, stemming =False, ctweets = True, number = False, hashtag = False))\n",
    "y_train  = train['HS']\n",
    "id_train = train['id']\n",
    "\n",
    "test_text  = dev['text'].map(lambda x: preprocessing(x, swords = False, url = True, stemming = False, ctweets = True, number = False, hashtag = False))\n",
    "y_test  = dev['HS']\n",
    "id_test = dev['id']\n",
    "\n",
    "data = np.concatenate((train_text, test_text), axis=0)\n",
    "classes = np.concatenate((y_train, y_test), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3783"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_train == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dF4yXFPKoA1g"
   },
   "outputs": [],
   "source": [
    "def word_embeddings(word_index, num_words, word_embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    \n",
    "    f = open('./glove.6B.300d.txt', 'r', encoding='utf-8')\n",
    "    \n",
    "    for line in tqdm(f):\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "    matrix = np.zeros((num_words, word_embedding_dim))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        \n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            matrix[i] = embedding_vector\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KyUJRLIQoSVj"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "max_features = 25000\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "filter_sizes = [3,4]\n",
    "num_filters = 512\n",
    "drop = 0.5\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(data)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(train_text)\n",
    "Y = tokenizer.texts_to_sequences(test_text)\n",
    "\n",
    "tweets = sequence.pad_sequences(X, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(Y, maxlen=maxlen)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "num_words = min(max_features, len(word_index) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BvKdMmt8wn-J"
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(tweets, y_train, test_size=0.1, random_state=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 64875,
     "status": "ok",
     "timestamp": 1547340476319,
     "user": {
      "displayName": "Alison Ribeiro",
      "photoUrl": "https://lh6.googleusercontent.com/-p2L5zKSs9Oc/AAAAAAAAAAI/AAAAAAAAAEE/PzwO-PqNro8/s64/photo.jpg",
      "userId": "16421844834226782203"
     },
     "user_tz": 120
    },
    "id": "3O5IjISOoGAV",
    "outputId": "66204646-f601-46ff-9552-9d4948350819"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:27, 14666.93it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "embedding_matrix = word_embeddings(word_index, num_words, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 781
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 60335,
     "status": "ok",
     "timestamp": 1547340652057,
     "user": {
      "displayName": "Alison Ribeiro",
      "photoUrl": "https://lh6.googleusercontent.com/-p2L5zKSs9Oc/AAAAAAAAAAI/AAAAAAAAAEE/PzwO-PqNro8/s64/photo.jpg",
      "userId": "16421844834226782203"
     },
     "user_tz": 120
    },
    "id": "pTZGv8lcWGIU",
    "outputId": "691014f5-fa16-4a7c-ab97-ac8b84fd74bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 100, 300)     5505600     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 100, 300, 1)  0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 98, 1, 512)   461312      reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 97, 1, 512)   614912      reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 512)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 512)    0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 2, 1, 512)    0           max_pooling2d_3[0][0]            \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 1024)         0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1024)         0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          524800      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            513         dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,107,137\n",
      "Trainable params: 7,107,137\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tweet_input = Input(shape=(maxlen,), dtype='int32')\n",
    "\n",
    "embedding = Embedding(num_words, embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=True)(tweet_input)\n",
    "\n",
    "reshape = Reshape((maxlen, embedding_dim, 1))(embedding)\n",
    "\n",
    "cnn1 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='tanh')(reshape)\n",
    "\n",
    "max1 = MaxPool2D(pool_size=(maxlen - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(cnn1)\n",
    "\n",
    "cnn2 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='tanh')(reshape)\n",
    "\n",
    "max2 = MaxPool2D(pool_size=(maxlen - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(cnn2)\n",
    "\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([max1, max2])\n",
    "\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "\n",
    "dropout = Dropout(drop)(flatten)\n",
    "\n",
    "dens = Dense(num_filters, activation='relu')(dropout)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(dens)\n",
    "\n",
    "model = Model(inputs=tweet_input, outputs=output)\n",
    "\n",
    "opt = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.01)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8100 samples, validate on 900 samples\n",
      "Epoch 1/20\n",
      "8100/8100 [==============================] - 3s 409us/step - loss: 0.9243 - acc: 0.6428 - val_loss: 0.5916 - val_acc: 0.6944\n",
      "Epoch 2/20\n",
      "8100/8100 [==============================] - 3s 361us/step - loss: 0.4829 - acc: 0.7714 - val_loss: 0.7651 - val_acc: 0.6044\n",
      "Epoch 3/20\n",
      "8100/8100 [==============================] - 3s 363us/step - loss: 0.4032 - acc: 0.8209 - val_loss: 0.5708 - val_acc: 0.7167\n",
      "Epoch 4/20\n",
      "8100/8100 [==============================] - 3s 361us/step - loss: 0.3457 - acc: 0.8502 - val_loss: 0.6028 - val_acc: 0.7167\n",
      "Epoch 5/20\n",
      "8100/8100 [==============================] - 3s 363us/step - loss: 0.3094 - acc: 0.8672 - val_loss: 0.5943 - val_acc: 0.7256\n",
      "Epoch 6/20\n",
      "8100/8100 [==============================] - 3s 362us/step - loss: 0.2722 - acc: 0.8915 - val_loss: 0.5956 - val_acc: 0.7400\n",
      "Epoch 7/20\n",
      "8100/8100 [==============================] - 3s 364us/step - loss: 0.2415 - acc: 0.9046 - val_loss: 0.6404 - val_acc: 0.7189\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "callbacks_list = [early_stopping,mcp_save]\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=20, shuffle=True, callbacks = callbacks_list,validation_data=(x_val, y_val))\n",
    "\n",
    "y_pred = (model.predict(x_test, batch_size=batch_size) > .5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 547,
     "status": "ok",
     "timestamp": 1547340661156,
     "user": {
      "displayName": "Alison Ribeiro",
      "photoUrl": "https://lh6.googleusercontent.com/-p2L5zKSs9Oc/AAAAAAAAAAI/AAAAAAAAAEE/PzwO-PqNro8/s64/photo.jpg",
      "userId": "16421844834226782203"
     },
     "user_tz": 120
    },
    "id": "rx8QHLaNWgdn",
    "outputId": "867d28f2-de4e-4807-93bc-3d96e3bdb6d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1.........: 0.698964\n",
      "Precision..: 0.717400\n",
      "Recall.....: 0.715865\n",
      "Accuracy...: 0.699000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, accuracy_score, recall_score\n",
    "\n",
    "print(\"F1.........: %f\" %(f1_score(y_test, y_pred, average=\"macro\")))\n",
    "print(\"Precision..: %f\" %(precision_score(y_test, y_pred, average=\"macro\")))\n",
    "print(\"Recall.....: %f\" %(recall_score(y_test, y_pred, average=\"macro\")))\n",
    "print(\"Accuracy...: %f\" %(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.800     0.623     0.701       573\n",
      "           1      0.610     0.792     0.689       427\n",
      "\n",
      "   micro avg      0.695     0.695     0.695      1000\n",
      "   macro avg      0.705     0.707     0.695      1000\n",
      "weighted avg      0.719     0.695     0.696      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9214898]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = preprocessing(\"i am going to build a wall for the immigrants\")\n",
    "x1 = [x1]\n",
    "y1 = tokenizer.texts_to_sequences(x1)\n",
    "tweets1 = sequence.pad_sequences(y1, maxlen=maxlen)\n",
    "model.predict(tweets1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_en_a.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
