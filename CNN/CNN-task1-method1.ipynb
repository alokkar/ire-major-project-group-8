{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1980,
     "status": "ok",
     "timestamp": 1547340412889,
     "user": {
      "displayName": "Alison Ribeiro",
      "photoUrl": "https://lh6.googleusercontent.com/-p2L5zKSs9Oc/AAAAAAAAAAI/AAAAAAAAAEE/PzwO-PqNro8/s64/photo.jpg",
      "userId": "16421844834226782203"
     },
     "user_tz": 120
    },
    "id": "QL28WSHoBsfp",
    "outputId": "f22d55d4-e94a-4a41-e258-28a2a2077a73"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPool2D, Reshape\n",
    "from keras.layers import Input, concatenate, Dropout\n",
    "from keras.layers import Embedding, Concatenate\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0s3Dnnk8FF4j"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train_en.tsv', delimiter='\\t',encoding='utf-8')\n",
    "dev = pd.read_csv('./dev_en.tsv', delimiter='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lrslbThJCeSY"
   },
   "outputs": [],
   "source": [
    "def clean_tweets(tweet):\n",
    "    tweet = re.sub('@(\\\\w{1,15})\\b', '', tweet)\n",
    "    tweet = tweet.replace(\"via \", \"\")\n",
    "    tweet = tweet.replace(\"RT \", \"\")\n",
    "    tweet = tweet.lower()\n",
    "    return tweet\n",
    "    \n",
    "def clean_url(tweet):\n",
    "    tweet = re.sub('http\\\\S+', '', tweet, flags=re.MULTILINE)   \n",
    "    return tweet\n",
    "    \n",
    "def remove_stop_words(tweet):\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    stops.update(['.',',','\"',\"'\",'?',':',';','(',')','[',']','{','}'])\n",
    "    toks = [tok for tok in tweet if not tok in stops and len(tok) >= 3]\n",
    "    return toks\n",
    "    \n",
    "def stemming_tweets(tweet):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed_words = [stemmer.stem(word) for word in tweet]\n",
    "    return stemmed_words\n",
    "\n",
    "def remove_number(tweet):\n",
    "    newTweet = re.sub('\\\\d+', '', tweet)\n",
    "    return newTweet\n",
    "\n",
    "def remove_hashtags(tweet):\n",
    "    result = ''\n",
    "\n",
    "    for word in tweet.split():\n",
    "        if word.startswith('#') or word.startswith('@'):\n",
    "            result += word[1:]\n",
    "            result += ' '\n",
    "        else:\n",
    "            result += word\n",
    "            result += ' '\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/aniket.joshi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QDsnLDRQGl71"
   },
   "outputs": [],
   "source": [
    "def preprocessing(tweet, swords = True, url = True, stemming = False, ctweets = True, number = True, hashtag = True):\n",
    "\n",
    "    if ctweets:\n",
    "        tweet = clean_tweets(tweet)\n",
    "\n",
    "    if url:\n",
    "        tweet = clean_url(tweet)\n",
    "\n",
    "    if hashtag:\n",
    "        tweet = remove_hashtags(tweet)\n",
    "    \n",
    "    twtk = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "    if number:\n",
    "        tweet = remove_number(tweet)\n",
    "    \n",
    "    tokens = [w.lower() for w in twtk.tokenize(tweet) if w != \"\" and w is not None]\n",
    "\n",
    "    if swords:\n",
    "        tokens = remove_stop_words(tokens)\n",
    "\n",
    "    if stemming:\n",
    "        tokens = stemming_tweets(tokens)\n",
    "\n",
    "    text = \" \".join(tokens)\n",
    "#     print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SunHFjyyFLR3"
   },
   "outputs": [],
   "source": [
    "train_text  = train['text'].map(lambda x: preprocessing(x, swords = True, url = True, stemming = False, ctweets = True, number = False, hashtag = False))\n",
    "y_train  = train['HS']\n",
    "id_train = train['id']\n",
    "ag_train = train['AG']\n",
    "test_text  = dev['text'].map(lambda x: preprocessing(x, swords = True, url = True, stemming = False, ctweets = True, number = False, hashtag = False))\n",
    "y_test  = dev['HS']\n",
    "id_test = dev['id']\n",
    "ag_test = dev['AG']\n",
    "\n",
    "data = np.concatenate((train_text, test_text), axis=0)\n",
    "classes = np.concatenate((y_train, y_test), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "for index,word in enumerate(train_text):\n",
    "#     print(y_train[index],ag_train[index])\n",
    "    if y_train[index]:\n",
    "        if ag_train[index]:\n",
    "            classes.append(0)\n",
    "#             print(0)\n",
    "        else:\n",
    "            classes.append(1)\n",
    "    else:\n",
    "        classes.append(2)\n",
    "\n",
    "y_train = classes\n",
    "classes = []\n",
    "for index,word in enumerate(test_text):\n",
    "    if y_test[index]:\n",
    "        if ag_test[index]:\n",
    "            classes.append(0)\n",
    "        else:\n",
    "            classes.append(1)\n",
    "    else:\n",
    "        classes.append(2)\n",
    "y_test = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "count1 = 0\n",
    "count2 = 0\n",
    "count3 = 0\n",
    "for x in y_train:\n",
    "    if x==0:\n",
    "        count1+=1\n",
    "    elif x == 1:\n",
    "        count2+=1\n",
    "    else:\n",
    "        count3+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1559, 2224, 5217)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count1,count2,count3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dF4yXFPKoA1g"
   },
   "outputs": [],
   "source": [
    "def word_embeddings(word_index, num_words, word_embedding_dim):\n",
    "    embeddings_index = {}\n",
    "    \n",
    "    f = open('./glove.6B.300d.txt', 'r', encoding='utf-8')\n",
    "    \n",
    "    for line in tqdm(f):\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    \n",
    "    f.close()\n",
    "\n",
    "    matrix = np.zeros((num_words, word_embedding_dim))\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        \n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            matrix[i] = embedding_vector\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KyUJRLIQoSVj"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "max_features = 25000\n",
    "maxlen = 40\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "filter_sizes = [3,4]\n",
    "num_filters = 512\n",
    "drop = 0.5\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(data)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(train_text)\n",
    "Y = tokenizer.texts_to_sequences(test_text)\n",
    "\n",
    "tweets = sequence.pad_sequences(X, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(Y, maxlen=maxlen)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "num_words = min(max_features, len(word_index) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BvKdMmt8wn-J"
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(tweets, y_train, test_size=0.1, random_state=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 64875,
     "status": "ok",
     "timestamp": 1547340476319,
     "user": {
      "displayName": "Alison Ribeiro",
      "photoUrl": "https://lh6.googleusercontent.com/-p2L5zKSs9Oc/AAAAAAAAAAI/AAAAAAAAAEE/PzwO-PqNro8/s64/photo.jpg",
      "userId": "16421844834226782203"
     },
     "user_tz": 120
    },
    "id": "3O5IjISOoGAV",
    "outputId": "66204646-f601-46ff-9552-9d4948350819"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:26, 14824.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "embedding_matrix = word_embeddings(word_index, num_words, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 781
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 60335,
     "status": "ok",
     "timestamp": 1547340652057,
     "user": {
      "displayName": "Alison Ribeiro",
      "photoUrl": "https://lh6.googleusercontent.com/-p2L5zKSs9Oc/AAAAAAAAAAI/AAAAAAAAAEE/PzwO-PqNro8/s64/photo.jpg",
      "userId": "16421844834226782203"
     },
     "user_tz": 120
    },
    "id": "pTZGv8lcWGIU",
    "outputId": "691014f5-fa16-4a7c-ab97-ac8b84fd74bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 40, 300)      5310000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 40, 300, 1)   0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 38, 1, 512)   461312      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 37, 1, 512)   614912      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 512)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 512)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2, 1, 512)    0           max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1024)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1024)         0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          524800      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            1539        dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,912,563\n",
      "Trainable params: 6,912,563\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tweet_input = Input(shape=(maxlen,), dtype='int32')\n",
    "\n",
    "embedding = Embedding(num_words, embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=True)(tweet_input)\n",
    "\n",
    "reshape = Reshape((maxlen, embedding_dim, 1))(embedding)\n",
    "\n",
    "cnn1 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='tanh')(reshape)\n",
    "\n",
    "max1 = MaxPool2D(pool_size=(maxlen - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(cnn1)\n",
    "\n",
    "cnn2 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='tanh')(reshape)\n",
    "\n",
    "max2 = MaxPool2D(pool_size=(maxlen - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(cnn2)\n",
    "\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([max1, max2])\n",
    "\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "\n",
    "dropout = Dropout(drop)(flatten)\n",
    "\n",
    "dens = Dense(num_filters, activation='relu')(dropout)\n",
    "\n",
    "output = Dense(3, activation='softmax')(dens)\n",
    "\n",
    "model = Model(inputs=tweet_input, outputs=output)\n",
    "\n",
    "opt = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.01)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8100 samples, validate on 900 samples\n",
      "Epoch 1/5\n",
      "8100/8100 [==============================] - 10s 1ms/step - loss: 1.8606 - acc: 0.5691 - val_loss: 0.9449 - val_acc: 0.6056\n",
      "Epoch 2/5\n",
      "8100/8100 [==============================] - 9s 1ms/step - loss: 0.7962 - acc: 0.6514 - val_loss: 0.8973 - val_acc: 0.5711\n",
      "Epoch 3/5\n",
      "8100/8100 [==============================] - 9s 1ms/step - loss: 0.6709 - acc: 0.7151 - val_loss: 0.7729 - val_acc: 0.6789\n",
      "Epoch 4/5\n",
      "8100/8100 [==============================] - 9s 1ms/step - loss: 0.5747 - acc: 0.7616 - val_loss: 0.7988 - val_acc: 0.6600\n",
      "Epoch 5/5\n",
      "8100/8100 [==============================] - 9s 1ms/step - loss: 0.4737 - acc: 0.8091 - val_loss: 0.7768 - val_acc: 0.6933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x153c02131400>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=5, shuffle=True, callbacks = callbacks_list,validation_data=(x_val, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 547,
     "status": "ok",
     "timestamp": 1547340661156,
     "user": {
      "displayName": "Alison Ribeiro",
      "photoUrl": "https://lh6.googleusercontent.com/-p2L5zKSs9Oc/AAAAAAAAAAI/AAAAAAAAAEE/PzwO-PqNro8/s64/photo.jpg",
      "userId": "16421844834226782203"
     },
     "user_tz": 120
    },
    "id": "rx8QHLaNWgdn",
    "outputId": "867d28f2-de4e-4807-93bc-3d96e3bdb6d0"
   },
   "outputs": [],
   "source": [
    "a = (model.predict(x_test, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0408656e-01, 2.2348827e-01, 6.7242521e-01],\n",
       "       [1.0056634e-02, 8.6935405e-03, 9.8124987e-01],\n",
       "       [2.7781087e-01, 5.4567772e-01, 1.7651132e-01],\n",
       "       [2.8649214e-01, 4.0063259e-01, 3.1287524e-01],\n",
       "       [3.7587533e-04, 4.6927592e-04, 9.9915481e-01],\n",
       "       [2.0718593e-02, 8.6163975e-02, 8.9311743e-01],\n",
       "       [1.1680706e-01, 2.0619969e-01, 6.7699319e-01],\n",
       "       [6.8230946e-03, 9.4031125e-02, 8.9914584e-01],\n",
       "       [3.3007205e-02, 1.3352007e-01, 8.3347273e-01],\n",
       "       [1.9852951e-01, 5.5519730e-01, 2.4627320e-01]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (a == a.max(axis=1)[:,None]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       ...,\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1.........: 0.519361\n",
      "Precision..: 0.516387\n",
      "Recall.....: 0.525887\n",
      "Accuracy...: 0.585000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, accuracy_score, recall_score\n",
    "\n",
    "print(\"F1.........: %f\" %(f1_score(y_test, y_pred, average=\"macro\")))\n",
    "print(\"Precision..: %f\" %(precision_score(y_test, y_pred, average=\"macro\")))\n",
    "print(\"Recall.....: %f\" %(recall_score(y_test, y_pred, average=\"macro\")))\n",
    "print(\"Accuracy...: %f\" %(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100  62  42]\n",
      " [ 58  88  77]\n",
      " [ 75 101 397]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "labels = [0, 1,2]\n",
    "cm = metrics.confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1), labels)\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.429     0.490     0.458       204\n",
      "           1      0.351     0.395     0.371       223\n",
      "           2      0.769     0.693     0.729       573\n",
      "\n",
      "   micro avg      0.585     0.585     0.585      1000\n",
      "   macro avg      0.516     0.526     0.519      1000\n",
      "weighted avg      0.607     0.585     0.594      1000\n",
      " samples avg      0.585     0.585     0.585      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15625197, 0.15428667, 0.6894614 ]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = preprocessing(\"I am going to kill them with a gun\")\n",
    "x1 = [x1]\n",
    "y1 = tokenizer.texts_to_sequences(x1)\n",
    "tweets1 = sequence.pad_sequences(y1, maxlen=maxlen)\n",
    "model.predict(tweets1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I swear I’m getting to places just in the nick of time! It’s exhausting @Sam_Schulman RT @MaireadEvvoMc: Made your bed, now lie in the flea pit you created 'Tourists go home, refugees welcome': why Barcelona chose migrants over visitors https://t.co/gtLMcuFC5p [0 0 1] [0. 0. 1.]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index,word in enumerate(dev['text']):\n",
    "    print(word,y_pred[index],y_test[index])\n",
    "    print(\"\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_en_a.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
