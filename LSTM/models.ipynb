{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,roc_auc_score,accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from keras.layers import Input, Concatenate\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Conv2D, MaxPool2D, Reshape\n",
    "from keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "import os, re, csv, math, codecs\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(tweet):\n",
    "    result = ''\n",
    "\n",
    "    for word in tweet.split():\n",
    "        if word.startswith('#') or word.startswith('@'):\n",
    "            result += word[1:]\n",
    "            result += ' '\n",
    "        else:\n",
    "            result += word\n",
    "            result += ' '\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer = SnowballStemmer('english')\n",
    "# stem_map={}\n",
    "\n",
    "# stopW = stopwords.words('english')\n",
    "# emoji_pattern = re.compile(\"[\"\n",
    "#      u\"\\U0001F600-\\U0001F64F\"  \n",
    "#      u\"\\U0001F300-\\U0001F5FF\"  \n",
    "#      u\"\\U0001F680-\\U0001F6FF\"  \n",
    "#      u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "#      u\"\\U00002702-\\U000027B0\"\n",
    "#      u\"\\U000024C2-\\U0001F251\"\n",
    "#      \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# def load_data(filename):\n",
    "#     n = ['id', 'text','HS','TR','AG']\n",
    "#     given_data = pd.read_csv(filename, sep='\\t',error_bad_lines=False, names=n, usecols=['text','HS','TR','AG'], skiprows=1)\n",
    "#     raw_data = given_data['text'].values\n",
    "#     labels_TR = list(map(int,given_data['TR'].values))\n",
    "#     labels_AG = list(map(int,given_data['AG'].values))\n",
    "#     labels_HS = list(map(int,given_data['HS'].values))\n",
    "#     return raw_data,labels_TR,labels_AG,labels_HS\n",
    "\n",
    "\n",
    "# def preprocess(tweet,pre_process = True,remove_URL = True, remove_mentions = True, remove_emojis = True, token_method = True, stem = True,remove_hashtag = True):\n",
    "\n",
    "#     if remove_URL:\n",
    "#         tweet = re.sub('http\\\\S+', '', tweet, flags=re.MULTILINE)\n",
    "    \n",
    "#     if remove_mentions:\n",
    "#         tweet = re.sub('@[^\\s]+','USER', tweet)\n",
    "    \n",
    "#     if pre_process:\n",
    "#         tweet = re.sub('@(\\\\w{1,15})\\b', '', tweet)\n",
    "#         tweet = tweet.replace(\"via \", \"\")\n",
    "#         tweet = tweet.replace(\"RT \", \"\")\n",
    "#         tweet = tweet.replace(\"ё\", \"е\")\n",
    "#         tweet = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', tweet)\n",
    "#         tweet = re.sub(' +',' ', tweet)\n",
    "#         tweet = tweet.lower()\n",
    "    \n",
    "#     if remove_emojis:    \n",
    "#         tweet = emoji_pattern.sub(r'', tweet)\n",
    "    \n",
    "#     if remove_hashtag:\n",
    "#         tweet = remove_hashtags(tweet)\n",
    "#     stemmed_text_token=[]\n",
    "    \n",
    "#     tokens = []\n",
    "#     if token_method:\n",
    "#         twtk = TweetTokenizer(reduce_len=True)\n",
    "#         tokens = twtk.tokenize(tweet)\n",
    "#     else:\n",
    "#         tokens = nltk.tokenize(tweet)\n",
    "        \n",
    "#     text_token = []\n",
    "#     for token in tokens:\n",
    "#         if token not in stop_words:\n",
    "#             text_token.append(token)\n",
    "#     for token in text_token:\n",
    "#         if token=='':\n",
    "#             continue\n",
    "#         elif token=='USER' or token == 'URL': \n",
    "#             stemmed_text_token.append(token)\n",
    "#         else:\n",
    "#             a=stem_map.get(token,0)\n",
    "#             if a==0:\n",
    "#                 a=stemmer.stem(token)\n",
    "#                 stem_map[token]=a\n",
    "#             if stem:\n",
    "#                 stemmed_text_token.append(a)\n",
    "#             else:\n",
    "#                 stemmed_text_token.append(token)\n",
    "                \n",
    "#     return ' '.join(stemmed_text_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "stem_map={}\n",
    "\n",
    "stopW = stopwords.words('english')\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "     u\"\\U0001F600-\\U0001F64F\"  \n",
    "     u\"\\U0001F300-\\U0001F5FF\"  \n",
    "     u\"\\U0001F680-\\U0001F6FF\"  \n",
    "     u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "     u\"\\U00002702-\\U000027B0\"\n",
    "     u\"\\U000024C2-\\U0001F251\"\n",
    "     \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def load_data(filename):\n",
    "    n = ['id', 'text','HS','TR','AG']\n",
    "    given_data = pd.read_csv(filename, sep='\\t',error_bad_lines=False, names=n, usecols=['text','HS','TR','AG'], skiprows=1)\n",
    "    raw_data = given_data['text'].values\n",
    "    labels_TR = list(map(int,given_data['TR'].values))\n",
    "    labels_AG = list(map(int,given_data['AG'].values))\n",
    "    labels_HS = list(map(int,given_data['HS'].values))\n",
    "    return raw_data,labels_TR,labels_AG,labels_HS\n",
    "\n",
    "def preprocess(tweet):\n",
    "    # ' '.join([word for word in tweet.spilt() ])\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', tweet)\n",
    "    tweet = re.sub('@[^\\s]+','USER', tweet)\n",
    "    tweet = tweet.replace(\"ё\", \"е\")\n",
    "    tweet = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', tweet)\n",
    "    tweet = re.sub(' +',' ', tweet)\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "    stemmed_text_token=[]\n",
    "    twtk = TweetTokenizer(reduce_len=True)\n",
    "#     tokens = tweet.split(' ')\n",
    "    tokens = twtk.tokenize(tweet)\n",
    "    text_token = []\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            text_token.append(token)\n",
    "    for token in text_token:\n",
    "        if token=='':\n",
    "            continue\n",
    "        elif token=='USER' or token=='URL': \n",
    "            stemmed_text_token.append(token)\n",
    "        \n",
    "        else:\n",
    "            a=stem_map.get(token,0)\n",
    "            if a==0:\n",
    "                a=stemmer.stem(token)\n",
    "                stem_map[token]=a\n",
    "            stemmed_text_token.append(a)\n",
    "    return ' '.join(stemmed_text_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = './train_en.tsv'\n",
    "train_raw_data,train_labels_TR,train_labels_AG,train_labels_HS = load_data(train_file)\n",
    "train_data = [preprocess(tweet) for tweet in train_raw_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_file = './dev_en.tsv'\n",
    "dev_raw_data,dev_labels_TR,dev_labels_AG,dev_labels_HS = load_data(dev_file)\n",
    "dev_data = [preprocess(tweet) for tweet in dev_raw_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hurray save us mani way USER USER lockthemup buildthewal enddaca boycottnfl boycottnik',\n",
       " 'whi would young fight age men vast major one escap war amp cannot fight like women children elder it major refuge actual refuge econom migrant tri get europ URL',\n",
       " 'USER illeg dump kid border like road kill refus unit they hope get amnesti free educ welfar illeg familesbelongtogeth countri taxpay dime it scam nodaca noamnesti sendth',\n",
       " 'ny time near all white state pose array problem immigr URL URL',\n",
       " 'orban brussel european leader ignor peopl want migrant URL']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i swear i get place nick time it exhaust USER rt USER made bed lie flea pit creat tourist go home refuge welcom barcelona chose migrant visitor URL',\n",
       " 'i immigr trump right immigr URL give insight stori URL',\n",
       " 'illegalimmigr illegalalien electoralsystem electoralcolleg i go shock peopl america not democraci america republ even defin america repres republ in true URL',\n",
       " 'USER we invas issu mexican buildthatwal',\n",
       " 'worker charg with sexual molest eight children immigr shelter URL']"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = train_data,train_labels_HS\n",
    "X_test,y_test = dev_data,dev_labels_HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data = np.concatenate((X_train, X_test), axis=0)\n",
    "# y_test = np_utils.to_categorical(y_test)\n",
    "# y_train = np_utils.to_categorical(y_train)\n",
    "# classes = np.concatenate((y_train, y_test), axis=0)\n",
    "y_test = np.array(y_test)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n"
     ]
    }
   ],
   "source": [
    "print('loading word embeddings...')\n",
    "embeddings_index = {}\n",
    "def load_embeddings(fileName):\n",
    "    f = codecs.open(fileName, encoding='utf-8')\n",
    "    for line in tqdm(f):\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('found %s word vectors' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training params\n",
    "batch_size = 256 \n",
    "num_epochs = 30 \n",
    "\n",
    "#model parameters\n",
    "num_filters = 64 \n",
    "embed_dim = 100 \n",
    "weight_decay = 1e-4\n",
    "MAX_NB_WORDS = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing input data...\n",
      "dictionary size:  13053\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 300\n",
    "print(\"tokenizing input data...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(X_train + X_test)  #leaky\n",
    "word_seq_train = tokenizer.texts_to_sequences(X_train)\n",
    "word_seq_test = tokenizer.texts_to_sequences(X_test)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size: \", len(word_index))\n",
    "\n",
    "#pad sequences\n",
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:17, 22425.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 400000 word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fileName = '../classification/glove.6B.100d.txt'\n",
    "embedding_matrix = load_embeddings(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix...\n",
      "number of null word embeddings: 5293\n"
     ]
    }
   ],
   "source": [
    "#embedding matrix\n",
    "print('preparing embedding matrix...')\n",
    "words_not_found = []\n",
    "length = len(word_index) + 1\n",
    "nb_words = min(MAX_NB_WORDS, length)\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training CNN ...\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_26 (Embedding)     (None, 300, 100)          1305400   \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 300, 64)           44864     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 100, 64)           28736     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 33, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 33, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 33, 64)            20544     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_18 (Glo (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,414,009\n",
      "Trainable params: 108,609\n",
      "Non-trainable params: 1,305,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_classes = 1\n",
    "print(\"training CNN ...\")\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words, embed_dim,\n",
    "          weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(num_filters, 3, activation='relu', padding='same'))\n",
    "model.add(Conv1D(num_filters, 5, activation='relu', padding='same'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))  #multi-label (k-hot encoding)\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, shuffle=True, validation_data=(x_val, y_val))\n",
    "\n",
    "# y_pred = (model.predict(x_test, batch_size=batch_size) > .5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8100 samples, validate on 900 samples\n",
      "Epoch 1/30\n",
      " - 5s - loss: 0.6819 - accuracy: 0.5633 - val_loss: 0.6616 - val_accuracy: 0.5889\n",
      "Epoch 2/30\n",
      " - 5s - loss: 0.6032 - accuracy: 0.6670 - val_loss: 0.9192 - val_accuracy: 0.3478\n",
      "Epoch 3/30\n",
      " - 5s - loss: 0.5497 - accuracy: 0.7269 - val_loss: 0.8460 - val_accuracy: 0.4533\n",
      "Epoch 4/30\n",
      " - 5s - loss: 0.4975 - accuracy: 0.7728 - val_loss: 0.8309 - val_accuracy: 0.5122\n",
      "Epoch 5/30\n",
      " - 5s - loss: 0.4386 - accuracy: 0.8102 - val_loss: 0.7917 - val_accuracy: 0.5678\n",
      "Epoch 6/30\n",
      " - 5s - loss: 0.3733 - accuracy: 0.8496 - val_loss: 0.9722 - val_accuracy: 0.5311\n",
      "Epoch 7/30\n",
      " - 5s - loss: 0.2737 - accuracy: 0.8965 - val_loss: 1.1969 - val_accuracy: 0.5222\n",
      "Epoch 8/30\n",
      " - 5s - loss: 0.2120 - accuracy: 0.9296 - val_loss: 1.5278 - val_accuracy: 0.5089\n",
      "Epoch 9/30\n",
      " - 5s - loss: 0.1295 - accuracy: 0.9610 - val_loss: 1.5470 - val_accuracy: 0.5367\n",
      "Epoch 10/30\n",
      " - 5s - loss: 0.0790 - accuracy: 0.9785 - val_loss: 2.3965 - val_accuracy: 0.5011\n",
      "Epoch 11/30\n",
      " - 5s - loss: 0.0621 - accuracy: 0.9833 - val_loss: 2.7580 - val_accuracy: 0.4911\n",
      "Epoch 12/30\n",
      " - 5s - loss: 0.0746 - accuracy: 0.9770 - val_loss: 2.2175 - val_accuracy: 0.5467\n",
      "Epoch 13/30\n",
      " - 5s - loss: 0.0721 - accuracy: 0.9772 - val_loss: 3.2194 - val_accuracy: 0.4378\n",
      "Epoch 14/30\n",
      " - 5s - loss: 0.0779 - accuracy: 0.9779 - val_loss: 2.2623 - val_accuracy: 0.5189\n",
      "Epoch 15/30\n",
      " - 5s - loss: 0.0328 - accuracy: 0.9915 - val_loss: 3.4596 - val_accuracy: 0.5156\n",
      "Epoch 16/30\n",
      " - 5s - loss: 0.0283 - accuracy: 0.9937 - val_loss: 5.0357 - val_accuracy: 0.4156\n",
      "Epoch 17/30\n",
      " - 5s - loss: 0.0408 - accuracy: 0.9890 - val_loss: 3.8092 - val_accuracy: 0.4589\n",
      "Epoch 18/30\n",
      " - 5s - loss: 0.0240 - accuracy: 0.9960 - val_loss: 3.5373 - val_accuracy: 0.5089\n",
      "Epoch 19/30\n",
      " - 5s - loss: 0.0201 - accuracy: 0.9956 - val_loss: 4.0024 - val_accuracy: 0.5211\n",
      "Epoch 20/30\n",
      " - 5s - loss: 0.0242 - accuracy: 0.9937 - val_loss: 3.3115 - val_accuracy: 0.5600\n",
      "Epoch 21/30\n",
      " - 5s - loss: 0.0211 - accuracy: 0.9954 - val_loss: 4.2753 - val_accuracy: 0.4811\n",
      "Epoch 22/30\n",
      " - 5s - loss: 0.0232 - accuracy: 0.9951 - val_loss: 3.7197 - val_accuracy: 0.5544\n",
      "Epoch 23/30\n",
      " - 5s - loss: 0.0384 - accuracy: 0.9899 - val_loss: 2.3345 - val_accuracy: 0.6256\n",
      "Epoch 24/30\n",
      " - 5s - loss: 0.0588 - accuracy: 0.9820 - val_loss: 3.3289 - val_accuracy: 0.5044\n",
      "Epoch 25/30\n",
      " - 5s - loss: 0.0360 - accuracy: 0.9904 - val_loss: 4.0982 - val_accuracy: 0.4656\n",
      "Epoch 26/30\n",
      " - 5s - loss: 0.0231 - accuracy: 0.9942 - val_loss: 3.9125 - val_accuracy: 0.5211\n",
      "Epoch 27/30\n",
      " - 5s - loss: 0.0156 - accuracy: 0.9965 - val_loss: 4.7640 - val_accuracy: 0.4956\n",
      "Epoch 28/30\n",
      " - 5s - loss: 0.0179 - accuracy: 0.9962 - val_loss: 4.5822 - val_accuracy: 0.5167\n",
      "Epoch 29/30\n",
      " - 5s - loss: 0.0239 - accuracy: 0.9942 - val_loss: 5.1745 - val_accuracy: 0.4722\n",
      "Epoch 30/30\n",
      " - 5s - loss: 0.0224 - accuracy: 0.9949 - val_loss: 4.5754 - val_accuracy: 0.5133\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "hist = model.fit(word_seq_train, y_train, batch_size=batch_size, epochs=num_epochs, validation_split=0.1, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample words not found:  ['ducation' 'alreday' 'oitnb' 'fricken' 'khammasi' 'boyf' 'nationoflaws'\n",
      " 'musicmakesadifference' 'hateposters' 'endcatchandrelease']\n"
     ]
    }
   ],
   "source": [
    "print(\"sample words not found: \", np.random.choice(words_not_found, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array 2 cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-207-d58c2c60ee1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precision\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Recall   \\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F1-Score \\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/classification/venv/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m   1567\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1569\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1570\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/classification/venv/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1413\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1415\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/classification/venv/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                          str(average_options))\n\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'binary'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/classification/venv/lib/python3.5/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/classification/venv/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \"\"\"\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/classification/venv/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \"\"\"\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/classification/venv/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[0;32m--> 146\u001b[0;31m                             \" a valid collection.\" % x)\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;31m# Check that shape is returning an integer or default to len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# Dask dataframes may not return numeric shape[0] value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Singleton array 2 cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "y_predict = model.predict(word_seq_test)\n",
    "\n",
    "y_predict = np.argmax(y_predict)\n",
    "y_test = np.argmax(y_test)\n",
    "\n",
    "print(\"Precision\\t\", precision_score(y_test, y_predict, average=None))\n",
    "print(\"Recall   \\t\", recall_score(y_test, y_predict, average=None))\n",
    "print(\"F1-Score \\t\", f1_score(y_test, y_predict, average=None))\n",
    "print(\"Accuracy\", accuracy_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
