{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "stopW = stopwords.words('english')\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "    u\"\\U00002702-\\U000027B0\"\n",
    "    u\"\\U000024C2-\\U0001F251\"\n",
    "\"]+\", flags=re.UNICODE)\n",
    "stem_map={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    n = ['id', 'text','HS','TR','AG']\n",
    "    given_data = pd.read_csv(filename, sep='\\t',error_bad_lines=False, names=n, usecols=['text','HS','TR','AG'], skiprows=1)\n",
    "    raw_data = given_data['text'].values\n",
    "    labels_TR = list(map(int,given_data['TR'].values))\n",
    "    labels_AG = list(map(int,given_data['AG'].values))\n",
    "    labels_HS = list(map(int,given_data['HS'].values))\n",
    "    \n",
    "    data = [preprocess(tweet) for tweet in raw_data]\n",
    "    X = []\n",
    "    y_AG = []\n",
    "    y_TR = []\n",
    "    \n",
    "    for index,word in enumerate(labels_HS):\n",
    "        if word:\n",
    "            X.append(data[index])\n",
    "            y_AG.append(labels_AG[index]) \n",
    "            y_TR.append(labels_TR[index])\n",
    "\n",
    "    \n",
    "    return X, y_AG, y_TR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet):\n",
    "    # ' '.join([word for word in tweet.spilt() ])\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', tweet)\n",
    "    tweet = re.sub('@[^\\s]+','USER', tweet)\n",
    "    tweet = tweet.replace(\"ё\", \"е\")\n",
    "    tweet = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', tweet)\n",
    "    tweet = re.sub(' +',' ', tweet)\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "    stemmed_text_token=[]\n",
    "    tokens = tweet.split(' ')\n",
    "    for token in tokens:\n",
    "        if token=='':\n",
    "            continue\n",
    "        elif token=='USER' or token=='URL': \n",
    "            stemmed_text_token.append(token)\n",
    "        # if token not in stopW:\n",
    "        #Need to check performance with and without stopwords.\n",
    "        else:\n",
    "            a=stem_map.get(token,0)\n",
    "            if a==0:\n",
    "                a=stemmer.stem(token)\n",
    "                stem_map[token]=a\n",
    "            stemmed_text_token.append(a)\n",
    "    return ' '.join(stemmed_text_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(data, labels):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.01, random_state=324)\n",
    "    clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=False,norm='l2')),\n",
    "                     ('clf', LogisticRegression(solver = 'newton-cg', penalty = 'l2'))])\n",
    "\n",
    "#     parameters = {\n",
    "#         'vect__ngram_range': [(1, 1), (1, 2),(2,2)],\n",
    "#         'tfidf__use_idf': (True, False),\n",
    "#         'tfidf__norm': ('l1', 'l2'),\n",
    "# #         'clf__penalty': ['l1', 'l2'],\n",
    "#         'clf__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "#     }\n",
    "#     clf = GridSearchCV(clf, parameters, cv=5, iid=False, n_jobs=-1)\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     print(clf.best_score_)\n",
    "#     for param_name in sorted(parameters.keys()):\n",
    "#         print(\"%s: %r\" % (param_name, clf.best_params_[param_name]))\n",
    "\n",
    "    clf.fit(X_train,y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(clf, X, y, print_results=True):\n",
    "    if print_results:\n",
    "        print(classification_report(y, clf.predict(X), digits=4))\n",
    "    return accuracy_score(y, clf.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all(clf_aggressive, clf_target, X_test, y_AG_test, y_TR_test, print_results=True):\n",
    "    print(\"Results aggressiveness on test data\")\n",
    "    accuracy_ag = test(clf_aggressive, X_test, y_AG_test, print_results=print_results)\n",
    "\n",
    "    print(\"Results targeting on test data\")\n",
    "    accuracy_tr = test(clf_target, X_test, y_TR_test, print_results=print_results)\n",
    "    \n",
    "    return accuracy_ag, accuracy_tr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Original Data\n",
      "\n",
      "Results aggressiveness on test data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6341    0.6996    0.6652       223\n",
      "           1     0.6298    0.5588    0.5922       204\n",
      "\n",
      "    accuracy                         0.6323       427\n",
      "   macro avg     0.6320    0.6292    0.6287       427\n",
      "weighted avg     0.6321    0.6323    0.6304       427\n",
      "\n",
      "Results targeting on test data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8435    0.9327    0.8858       208\n",
      "           1     0.9289    0.8356    0.8798       219\n",
      "\n",
      "    accuracy                         0.8829       427\n",
      "   macro avg     0.8862    0.8842    0.8828       427\n",
      "weighted avg     0.8873    0.8829    0.8827       427\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuray_tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-7d98b4bddf91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0maccuracy_ag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_aggressive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_AG_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_TR_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtransl_ag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maccuracy_ag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtransl_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maccuray_tr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0meda_ag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maccuracy_ag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0meda_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maccuracy_tr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuray_tr' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=====================================================\")\n",
    "training_file = '../Dataset/train_en.tsv'\n",
    "testing_file = '../Dataset/dev_en.tsv'\n",
    "\n",
    "X_train, y_AG_train, y_TR_train = load_data(training_file)\n",
    "X_test, y_AG_test, y_TR_test = load_data(testing_file)\n",
    "\n",
    "clf_aggressive = classifier(X_train, y_AG_train)\n",
    "clf_target = classifier(X_train, y_TR_train)\n",
    "\n",
    "print(\"Original Data\")\n",
    "print()\n",
    "accuracy_ag, accuracy_tr = test_all(clf_aggressive, clf_target, X_test, y_AG_test, y_TR_test)\n",
    "transl_ag = [accuracy_ag]\n",
    "transl_tr = [accuracy_tr]\n",
    "eda_ag = [accuracy_ag]\n",
    "eda_tr = [accuracy_tr]\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"Data Augmentation using EDA technique\")\n",
    "for degree in range(3):\n",
    "    print(\"------------------------------------------------\")\n",
    "    print(\"DATA*{}\".format(degree+2))\n",
    "    filename = '../Dataset/eda_aug/data_degree_{}.tsv'.format(degree+2)\n",
    "    X_train, y_AG_train, y_TR_train = load_data(filename)\n",
    "    clf_aggressive = classifier(X_train, y_AG_train)\n",
    "    clf_target = classifier(X_train, y_TR_train)\n",
    "    accuracy_ag, accuracy_tr = test_all(clf_aggressive, clf_target, X_test, y_AG_test, y_TR_test)\n",
    "    eda_ag.append(accuracy_ag)\n",
    "    eda_tr.append(accuracy_tr)\n",
    "print(\"=====================================================\")\n",
    "print(\"Data Augmentation using Machine Translation\")\n",
    "for degree in range(3):\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(\"DATA*{}\".format(degree+2))\n",
    "    filename = '../Dataset/transl_aug/data_degree_{}.tsv'.format(degree+2)\n",
    "    X_train, y_AG_train, y_TR_train = load_data(filename)\n",
    "    clf_aggressive = classifier(X_train, y_AG_train)\n",
    "    clf_target = classifier(X_train, y_TR_train)\n",
    "    accuracy_ag, accuracy_tr = test_all(clf_aggressive, clf_target, X_test, y_AG_test, y_TR_test)\n",
    "    transl_ag.append(accuracy_ag)\n",
    "    transl_tr.append(accuracy_tr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(results_eda, results_tr, plot_title, yrange):\n",
    "    x = range(1,5)\n",
    "    plt.plot(x, results_eda)\n",
    "    plt.plot(x, results_tr)\n",
    "    plt.xticks(x, x)\n",
    "    plt.xlabel(\"Size of Dataset\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.ylim(yrange[0], yrange[1])\n",
    "    plt.title(plot_title)\n",
    "    plt.legend(['EDA', 'Machine Translation'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(eda_ag, transl_ag, \"Aggression results after data augmentation\", [0.6, 0.7])\n",
    "make_plot(eda_tr, transl_tr, \"Target results after data augmentation\", [0.88, 0.92])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smai_p3",
   "language": "python",
   "name": "smai_p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
