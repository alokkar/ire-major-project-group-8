{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "stopW = stopwords.words('english')\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "    u\"\\U00002702-\\U000027B0\"\n",
    "    u\"\\U000024C2-\\U0001F251\"\n",
    "\"]+\", flags=re.UNICODE)\n",
    "stem_map={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    n = ['id', 'text','HS','TR','AG']\n",
    "    given_data = pd.read_csv(filename, sep='\\t',error_bad_lines=False, names=n, usecols=['text','HS','TR','AG'], skiprows=1)\n",
    "    raw_data = given_data['text'].values\n",
    "    labels_TR = list(map(int,given_data['TR'].values))\n",
    "    labels_AG = list(map(int,given_data['AG'].values))\n",
    "    labels_HS = list(map(int,given_data['HS'].values))\n",
    "    \n",
    "    data = [preprocess(tweet) for tweet in raw_data]\n",
    "    X = []\n",
    "    y_AG = []\n",
    "    y_TR = []\n",
    "    \n",
    "    for index,word in enumerate(labels_HS):\n",
    "        if word:\n",
    "            X.append(data[index])\n",
    "            y_AG.append(labels_AG[index]) \n",
    "            y_TR.append(labels_TR[index])\n",
    "\n",
    "    \n",
    "    return X, y_AG, y_TR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet):\n",
    "    # ' '.join([word for word in tweet.spilt() ])\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', tweet)\n",
    "    tweet = re.sub('@[^\\s]+','USER', tweet)\n",
    "    tweet = tweet.replace(\"ё\", \"е\")\n",
    "    tweet = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', tweet)\n",
    "    tweet = re.sub(' +',' ', tweet)\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "    stemmed_text_token=[]\n",
    "    tokens = tweet.split(' ')\n",
    "    for token in tokens:\n",
    "        if token=='':\n",
    "            continue\n",
    "        elif token=='USER' or token=='URL': \n",
    "            stemmed_text_token.append(token)\n",
    "        # if token not in stopW:\n",
    "        #Need to check performance with and without stopwords.\n",
    "        else:\n",
    "            a=stem_map.get(token,0)\n",
    "            if a==0:\n",
    "                a=stemmer.stem(token)\n",
    "                stem_map[token]=a\n",
    "            stemmed_text_token.append(a)\n",
    "    return ' '.join(stemmed_text_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(data, labels):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.01, random_state=324)\n",
    "    clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=False,norm='l2')),\n",
    "                     ('clf', LogisticRegression(penalty = 'l2'))])\n",
    "\n",
    "    parameters = {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2),(2,2)],\n",
    "        'tfidf__use_idf': (True, False),\n",
    "        'tfidf__norm': ('l1', 'l2'),\n",
    "        'clf__alpha': [10,1, 1e-1, 1e-2,1e-3]\n",
    "    }\n",
    "#     clf = GridSearchCV(clf, parameters, cv=5, iid=False, n_jobs=-1)\n",
    "\n",
    "    clf.fit(X_train,y_train)\n",
    "    prediction = clf.predict(X_test)\n",
    "#     accuracy_score = text_clf.score(X_test,y_test)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X, Y, clf):\n",
    "    print(classification_report(Y, clf.predict(X), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    training_file = 'train_en.tsv'\n",
    "    testing_file = 'dev_en.tsv'\n",
    "    \n",
    "    X_train, y_AG_train, y_TR_train = load_data(training_file)\n",
    "    X_test, y_AG_test, y_TR_test = load_data(testing_file)\n",
    "    \n",
    "    clf_aggresive = classifier(X_train, y_AG_train)\n",
    "    clf_target = classifier(X_train, y_TR_train)\n",
    "    \n",
    "    print(\"Results aggresiveness on training data\")\n",
    "    test(clf_aggresive, X_train, y_AG_train)\n",
    "\n",
    "    print(\"Results aggresiveness on test data\")\n",
    "    test(clf_aggresive, X_test, y_AG_test)\n",
    "\n",
    "    print(\"Results targeting on training data\")\n",
    "    test(clf_target, X_train, y_TR_train)\n",
    "\n",
    "    print(\"Results targeting on test data\")\n",
    "    test(clf_target, X_test, y_TR_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanujgarg/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results aggresiveness on training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanujgarg/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4be4dd58a851>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-a0e440463bf4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Results aggresiveness on training data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_aggresive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_AG_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Results aggresiveness on test data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-96ea6f3ee3ff>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(X, Y, clf)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smai_p3",
   "language": "python",
   "name": "smai_p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
