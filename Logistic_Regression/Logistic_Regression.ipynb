{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "stopW = stopwords.words('english')\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "    u\"\\U00002702-\\U000027B0\"\n",
    "    u\"\\U000024C2-\\U0001F251\"\n",
    "\"]+\", flags=re.UNICODE)\n",
    "stem_map={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    n = ['id', 'text','HS','TR','AG']\n",
    "    given_data = pd.read_csv(filename, sep='\\t',error_bad_lines=False, names=n, usecols=['text','HS','TR','AG'], skiprows=1)\n",
    "    raw_data = given_data['text'].values\n",
    "    labels_TR = list(map(int,given_data['TR'].values))\n",
    "    labels_AG = list(map(int,given_data['AG'].values))\n",
    "    labels_HS = list(map(int,given_data['HS'].values))\n",
    "    \n",
    "    data = [preprocess(tweet) for tweet in raw_data]\n",
    "    X = []\n",
    "    y_AG = []\n",
    "    y_TR = []\n",
    "    \n",
    "    for index,word in enumerate(labels_HS):\n",
    "        if word:\n",
    "            X.append(data[index])\n",
    "            y_AG.append(labels_AG[index]) \n",
    "            y_TR.append(labels_TR[index])\n",
    "\n",
    "    \n",
    "    return X, y_AG, y_TR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tweet):\n",
    "    # ' '.join([word for word in tweet.spilt() ])\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', tweet)\n",
    "    tweet = re.sub('@[^\\s]+','USER', tweet)\n",
    "    tweet = tweet.replace(\"ё\", \"е\")\n",
    "    tweet = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', tweet)\n",
    "    tweet = re.sub(' +',' ', tweet)\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "    stemmed_text_token=[]\n",
    "    tokens = tweet.split(' ')\n",
    "    for token in tokens:\n",
    "        if token=='':\n",
    "            continue\n",
    "        elif token=='USER' or token=='URL': \n",
    "            stemmed_text_token.append(token)\n",
    "        # if token not in stopW:\n",
    "        #Need to check performance with and without stopwords.\n",
    "        else:\n",
    "            a=stem_map.get(token,0)\n",
    "            if a==0:\n",
    "                a=stemmer.stem(token)\n",
    "                stem_map[token]=a\n",
    "            stemmed_text_token.append(a)\n",
    "    return ' '.join(stemmed_text_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(data, labels):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.01, random_state=324)\n",
    "    clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=False,norm='l2')),\n",
    "                     ('clf', LogisticRegression(solver = 'newton-cg', penalty = 'l2'))])\n",
    "\n",
    "#     parameters = {\n",
    "#         'vect__ngram_range': [(1, 1), (1, 2),(2,2)],\n",
    "#         'tfidf__use_idf': (True, False),\n",
    "#         'tfidf__norm': ('l1', 'l2'),\n",
    "# #         'clf__penalty': ['l1', 'l2'],\n",
    "#         'clf__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "#     }\n",
    "#     clf = GridSearchCV(clf, parameters, cv=5, iid=False, n_jobs=-1)\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     print(clf.best_score_)\n",
    "#     for param_name in sorted(parameters.keys()):\n",
    "#         print(\"%s: %r\" % (param_name, clf.best_params_[param_name]))\n",
    "\n",
    "    clf.fit(X_train,y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(clf, X, Y):\n",
    "    print(classification_report(Y, clf.predict(X), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    training_file = 'train_en.tsv'\n",
    "    testing_file = 'dev_en.tsv'\n",
    "    \n",
    "    X_train, y_AG_train, y_TR_train = load_data(training_file)\n",
    "    X_test, y_AG_test, y_TR_test = load_data(testing_file)\n",
    "    \n",
    "    clf_aggresive = classifier(X_train, y_AG_train)\n",
    "    clf_target = classifier(X_train, y_TR_train)\n",
    "    \n",
    "    print(\"Results aggresiveness on training data\")\n",
    "    test(clf_aggresive, X_train, y_AG_train)\n",
    "\n",
    "    print(\"Results aggresiveness on test data\")\n",
    "    test(clf_aggresive, X_test, y_AG_test)\n",
    "\n",
    "    print(\"Results targeting on training data\")\n",
    "    test(clf_target, X_train, y_TR_train)\n",
    "\n",
    "    print(\"Results targeting on test data\")\n",
    "    test(clf_target, X_test, y_TR_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results aggresiveness on training data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8046    0.9218    0.8592      2224\n",
      "           1     0.8591    0.6806    0.7595      1559\n",
      "\n",
      "    accuracy                         0.8224      3783\n",
      "   macro avg     0.8318    0.8012    0.8093      3783\n",
      "weighted avg     0.8270    0.8224    0.8181      3783\n",
      "\n",
      "Results aggresiveness on test data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6341    0.6996    0.6652       223\n",
      "           1     0.6298    0.5588    0.5922       204\n",
      "\n",
      "    accuracy                         0.6323       427\n",
      "   macro avg     0.6320    0.6292    0.6287       427\n",
      "weighted avg     0.6321    0.6323    0.6304       427\n",
      "\n",
      "Results targeting on training data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9134    0.9369    0.9250      2442\n",
      "           1     0.8795    0.8382    0.8583      1341\n",
      "\n",
      "    accuracy                         0.9019      3783\n",
      "   macro avg     0.8964    0.8876    0.8917      3783\n",
      "weighted avg     0.9014    0.9019    0.9014      3783\n",
      "\n",
      "Results targeting on test data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8435    0.9327    0.8858       208\n",
      "           1     0.9289    0.8356    0.8798       219\n",
      "\n",
      "    accuracy                         0.8829       427\n",
      "   macro avg     0.8862    0.8842    0.8828       427\n",
      "weighted avg     0.8873    0.8829    0.8827       427\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liblinear\n",
    "# 0.6774365821094793\n",
    "# clf__penalty: 'l1'\n",
    "# tfidf__norm: 'l2'\n",
    "# tfidf__use_idf: True\n",
    "# vect__ngram_range: (1, 1)\n",
    "# 0.8646110992910334\n",
    "# clf__penalty: 'l2'\n",
    "# tfidf__norm: 'l2'\n",
    "# tfidf__use_idf: False\n",
    "# vect__ngram_range: (1, 2)\n",
    "\n",
    "#l2\n",
    "# 0.6742323097463284\n",
    "# clf__solver: 'newton-cg'\n",
    "# tfidf__norm: 'l2'\n",
    "# tfidf__use_idf: True\n",
    "# vect__ngram_range: (1, 2)\n",
    "# 0.8646110992910334\n",
    "# clf__solver: 'newton-cg'\n",
    "# tfidf__norm: 'l2'\n",
    "# tfidf__use_idf: False\n",
    "# vect__ngram_range: (1, 2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smai_p3",
   "language": "python",
   "name": "smai_p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
