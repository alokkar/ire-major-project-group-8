{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# from nltk.corpus import stopwords as stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', \n",
    "            'ours', 'ourselves', 'you', 'your', 'yours', \n",
    "            'yourself', 'yourselves', 'he', 'him', 'his', \n",
    "            'himself', 'she', 'her', 'hers', 'herself', \n",
    "            'it', 'its', 'itself', 'they', 'them', 'their', \n",
    "            'theirs', 'themselves', 'what', 'which', 'who', \n",
    "            'whom', 'this', 'that', 'these', 'those', 'am', \n",
    "            'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
    "            'have', 'has', 'had', 'having', 'do', 'does', 'did',\n",
    "            'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
    "            'because', 'as', 'until', 'while', 'of', 'at', \n",
    "            'by', 'for', 'with', 'about', 'against', 'between',\n",
    "            'into', 'through', 'during', 'before', 'after', \n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in',\n",
    "            'out', 'on', 'off', 'over', 'under', 'again', \n",
    "            'further', 'then', 'once', 'here', 'there', 'when', \n",
    "            'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "            'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "            'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', \n",
    "            'very', 's', 't', 'can', 'will', 'just', 'don', \n",
    "'should', 'now', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word): \n",
    "        for l in syn.lemmas(): \n",
    "            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "            synonyms.add(synonym) \n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    return list(synonyms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_replacement(words, n):\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            #print(\"replaced\", random_word, \"with\", synonym)\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n: #only replace up to n words\n",
    "            break\n",
    "\n",
    "    #this is stupid but we need it, trust me\n",
    "    sentence = ' '.join(new_words)\n",
    "    new_words = sentence.split(' ')\n",
    "\n",
    "    return new_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_deletion(words, p):\n",
    "\n",
    "    #obviously, if there's only one word, don't delete it\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "\n",
    "    #randomly delete words with probability p\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        r = random.uniform(0, 1)\n",
    "        if r > p:\n",
    "            new_words.append(word)\n",
    "\n",
    "    #if you end up deleting all words, just return a random word\n",
    "    if len(new_words) == 0:\n",
    "        rand_int = random.randint(0, len(words)-1)\n",
    "        return [words[rand_int]]\n",
    "\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_word(new_words):\n",
    "    random_idx_1 = random.randint(0, len(new_words)-1)\n",
    "    random_idx_2 = random_idx_1\n",
    "    counter = 0\n",
    "    while random_idx_2 == random_idx_1:\n",
    "        random_idx_2 = random.randint(0, len(new_words)-1)\n",
    "        counter += 1\n",
    "        if counter > 3:\n",
    "            return new_words\n",
    "    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_swap(words, n):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        new_words = swap_word(new_words)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_word(new_words):\n",
    "    synonyms = []\n",
    "    counter = 0\n",
    "    while len(synonyms) < 1:\n",
    "        random_word = new_words[random.randint(0, len(new_words)-1)]\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        counter += 1\n",
    "        if counter >= 10:\n",
    "            return\n",
    "    random_synonym = synonyms[0]\n",
    "    random_idx = random.randint(0, len(new_words)-1)\n",
    "    new_words.insert(random_idx, random_synonym)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_insertion(words, n):\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        add_word(new_words)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_chars(line):\n",
    "\n",
    "    clean_line = \"\"\n",
    "\n",
    "    line = line.replace(\"â€™\", \"\")\n",
    "    line = line.replace(\"'\", \"\")\n",
    "    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n",
    "    line = line.replace(\"\\t\", \" \")\n",
    "    line = line.replace(\"\\n\", \" \")\n",
    "    line = line.lower()\n",
    "\n",
    "    for char in line:\n",
    "        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n",
    "            clean_line += char\n",
    "        else:\n",
    "            clean_line += ' '\n",
    "\n",
    "    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n",
    "    if clean_line[0] == ' ':\n",
    "        clean_line = clean_line[1:]\n",
    "    return clean_line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n",
    "    sentence = get_only_chars(sentence)\n",
    "    words = sentence.split(' ')\n",
    "    words = [word for word in words if word is not '']\n",
    "    num_words = len(words)\n",
    "    \n",
    "    augmented_sentences = []\n",
    "    num_new_per_technique = int(num_aug/4)+1\n",
    "    n_sr = max(1, int(alpha_sr*num_words))\n",
    "    n_ri = max(1, int(alpha_ri*num_words))\n",
    "    n_rs = max(1, int(alpha_rs*num_words))\n",
    "\n",
    "    #sr\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = synonym_replacement(words, n_sr)\n",
    "        augmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "    #ri\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = random_insertion(words, n_ri)\n",
    "        augmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "    #rs\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = random_swap(words, n_rs)\n",
    "        augmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "    #rd\n",
    "    for _ in range(num_new_per_technique):\n",
    "        a_words = random_deletion(words, p_rd)\n",
    "        augmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "    augmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n",
    "    random.shuffle(augmented_sentences)\n",
    "\n",
    "    #trim so that we have the desired number of augmented sentences\n",
    "    if num_aug >= 1:\n",
    "        augmented_sentences = augmented_sentences[:num_aug]\n",
    "    else:\n",
    "        keep_prob = num_aug / len(augmented_sentences)\n",
    "        augmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n",
    "\n",
    "    #append the original sentence\n",
    "    augmented_sentences.append(sentence)\n",
    "\n",
    "    return augmented_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_in_file(file_writer, row, cur_id, new_text):\n",
    "    hs = row['HS']\n",
    "    tr = row['TR']\n",
    "    ag = row['AG']\n",
    "    file_writer.writerow([cur_id, new_text, hs, tr, ag])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(filename, num_aug, new_filename):\n",
    "    n = ['id', 'text','HS','TR','AG']\n",
    "    given_data = pd.read_csv(filename, sep='\\t',error_bad_lines=False, names=n, usecols=['text','HS','TR','AG'], skiprows=1)\n",
    "    with open(new_filename, 'wt') as out_file:\n",
    "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "        tsv_writer.writerow(['id', 'text', 'HS', 'TR', 'AG'])\n",
    "        cur_id = 1\n",
    "        \n",
    "        for index, row in tqdm(given_data.iterrows()):\n",
    "            write_in_file(tsv_writer, row, cur_id, row['text'])\n",
    "            cur_id += 1\n",
    "\n",
    "            augmented_data = eda(row['text'], num_aug=num_aug)\n",
    "            for data in augmented_data:\n",
    "                write_in_file(tsv_writer, row, cur_id, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbad76c36f394a10a38bb726a33dc5ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df876b6b7f5f4168972818c386aba5f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956178d682d946168e9f4ffa36ccd874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for degree in range(3):\n",
    "    augment_data('Logistic_Regression/train_en.tsv',degree, 'data_degree_{}.tsv'.format(degree))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smai_p3",
   "language": "python",
   "name": "smai_p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
