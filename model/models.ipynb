{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,roc_auc_score,accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "stem_map={}\n",
    "\n",
    "stopW = stopwords.words('english')\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "     u\"\\U0001F600-\\U0001F64F\"  \n",
    "     u\"\\U0001F300-\\U0001F5FF\"  \n",
    "     u\"\\U0001F680-\\U0001F6FF\"  \n",
    "     u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "     u\"\\U00002702-\\U000027B0\"\n",
    "     u\"\\U000024C2-\\U0001F251\"\n",
    "     \"]+\", flags=re.UNICODE)\n",
    "\n",
    "def load_data(filename):\n",
    "    n = ['id', 'text','HS','TR','AG']\n",
    "    given_data = pd.read_csv(filename, sep='\\t',error_bad_lines=False, names=n, usecols=['text','HS','TR','AG'], skiprows=1)\n",
    "    raw_data = given_data['text'].values\n",
    "    labels_TR = list(map(int,given_data['TR'].values))\n",
    "    labels_AG = list(map(int,given_data['AG'].values))\n",
    "    labels_HS = list(map(int,given_data['HS'].values))\n",
    "    return raw_data,labels_TR,labels_AG,labels_HS\n",
    "\n",
    "def preprocess(tweet):\n",
    "    # ' '.join([word for word in tweet.spilt() ])\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', tweet)\n",
    "    tweet = re.sub('@[^\\s]+','USER', tweet)\n",
    "    tweet = tweet.replace(\"ё\", \"е\")\n",
    "    tweet = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', tweet)\n",
    "    tweet = re.sub(' +',' ', tweet)\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "    stemmed_text_token=[]\n",
    "    twtk = TweetTokenizer(reduce_len=True)\n",
    "#     tokens = tweet.split(' ')\n",
    "    tokens = twtk.tokenize(tweet)\n",
    "    text_token = []\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            text_token.append(token)\n",
    "    for token in text_token:\n",
    "        if token=='':\n",
    "            continue\n",
    "        elif token=='USER' or token=='URL': \n",
    "            stemmed_text_token.append(token)\n",
    "        \n",
    "        else:\n",
    "            a=stem_map.get(token,0)\n",
    "            if a==0:\n",
    "                a=stemmer.stem(token)\n",
    "                stem_map[token]=a\n",
    "            stemmed_text_token.append(a)\n",
    "    return ' '.join(stemmed_text_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './train_en.tsv'\n",
    "raw_data,labels_TR,labels_AG,labels_HS = load_data(filename)\n",
    "\n",
    "data = [preprocess(tweet) for tweet in raw_data]\n",
    "# classifier(data,labels_HS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data, labels_HS, test_size=0.1, random_state=324)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "y_test[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "y_train = np_utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "import os, re, csv, math, codecs\n",
    "from sklearn.preprocessing import LabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 100000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n"
     ]
    }
   ],
   "source": [
    "print('loading word embeddings...')\n",
    "embeddings_index = {}\n",
    "def load_embeddings():\n",
    "    f = codecs.open('../classification/glove.6B.100d.txt', encoding='utf-8')\n",
    "    for line in tqdm(f):\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('found %s word vectors' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:23, 16798.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 400000 word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "load_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing input data...\n",
      "dictionary size:  12288\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 300\n",
    "print(\"tokenizing input data...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(x_train + x_test)  #leaky\n",
    "word_seq_train = tokenizer.texts_to_sequences(x_train)\n",
    "word_seq_test = tokenizer.texts_to_sequences(x_test)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size: \", len(word_index))\n",
    "\n",
    "#pad sequences\n",
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training params\n",
    "batch_size = 128 \n",
    "num_epochs = 20 \n",
    "\n",
    "#model parameters\n",
    "num_filters = 64 \n",
    "embed_dim = 100 \n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix...\n",
      "number of null word embeddings: 4919\n"
     ]
    }
   ],
   "source": [
    "#embedding matrix\n",
    "print('preparing embedding matrix...')\n",
    "words_not_found = []\n",
    "length = len(word_index) + 1\n",
    "nb_words = min(MAX_NB_WORDS, length)\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample words not found:  ['moreov' 'anncoult' 'trifl' 'wherertherefuge' 'goodmorn' 'bitchi' 'movi'\n",
      " 'therippleeffect' 'masquerad' 'strach']\n"
     ]
    }
   ],
   "source": [
    "print(\"sample words not found: \", np.random.choice(words_not_found, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training CNN ...\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 300, 100)          1228900   \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 300, 64)           44864     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 100, 64)           28736     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 33, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 33, 64)            12352     \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 33, 64)            20544     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_13 (Glo (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,337,542\n",
      "Trainable params: 108,642\n",
      "Non-trainable params: 1,228,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "print(\"training CNN ...\")\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words, embed_dim,\n",
    "          weights=[embedding_matrix], input_length=max_seq_len, trainable=False))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Conv1D(num_filters, 3, activation='relu', padding='same'))\n",
    "model.add(Conv1D(num_filters, 5, activation='relu', padding='same'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))  #multi-label (k-hot encoding)\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
    "callbacks_list = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7290 samples, validate on 810 samples\n",
      "Epoch 1/20\n",
      " - 5s - loss: 0.6478 - accuracy: 0.6095 - val_loss: 0.6023 - val_accuracy: 0.6679\n",
      "Epoch 2/20\n",
      " - 5s - loss: 0.5762 - accuracy: 0.6918 - val_loss: 0.5625 - val_accuracy: 0.7123\n",
      "Epoch 3/20\n",
      " - 5s - loss: 0.5226 - accuracy: 0.7429 - val_loss: 0.5542 - val_accuracy: 0.7142\n",
      "Epoch 4/20\n",
      " - 5s - loss: 0.4575 - accuracy: 0.7873 - val_loss: 0.5533 - val_accuracy: 0.7222\n",
      "Epoch 5/20\n",
      " - 5s - loss: 0.3675 - accuracy: 0.8420 - val_loss: 0.6832 - val_accuracy: 0.6778\n",
      "Epoch 6/20\n",
      " - 5s - loss: 0.2671 - accuracy: 0.8953 - val_loss: 0.6834 - val_accuracy: 0.7247\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "#model training\n",
    "hist = model.fit(word_seq_train, y_train, batch_size=batch_size, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.1, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision\t [0.74115456 0.67493113]\n",
      "Recall   \t [0.77131783 0.63802083]\n",
      "F1-Score \t [0.75593542 0.65595716]\n"
     ]
    }
   ],
   "source": [
    "y_predict = model.predict(word_seq_test)\n",
    "\n",
    "y_predict = np.argmax(y_predict, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"Precision\\t\", precision_score(y_test, y_predict, average=None))\n",
    "print(\"Recall   \\t\", recall_score(y_test, y_predict, average=None))\n",
    "print(\"F1-Score \\t\", f1_score(y_test, y_predict, average=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC  \t 0.7046693313953489\n"
     ]
    }
   ],
   "source": [
    "print(\"ROC-AUC  \\t\", roc_auc_score(y_test, y_predict, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.7144444444444444\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy\", accuracy_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
